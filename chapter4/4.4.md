## Metrics with Purpose
## 度量指标
---

Chapter 5 covers how to monitor and alert using SLI metrics when a system’s error budget is under threat. SLI metrics are the first metrics you want to check when SLO- based alerts trigger. These metrics should appear prominently in your service’s dashboard, ideally on its landing page.

第5章介绍了当系统出现错误受到威胁时如何使用SLI指标进行监控和警报。SLI指标是您在基于SLO的警报触发时要检查的第一个指标。这些指标应显示在服务的信息中心中，最好位于其登陆页上。

When investigating the cause of an SLO violation, you will most likely not get enough information from the SLO dashboards. These dashboards show that you are violating the SLO, but not necessarily why. What other data should the monitoring dashboards display?

在排查SLO触发的原因时，您很可能无法从SLO仪表板获得足够的信息。这些仪表板显示您触发了SLO阀值，但不一定知道为什么。监控仪表板应该显示哪些数据呢？

We’ve found the following guidelines helpful in implementing metrics. These metrics should provide reasonable monitoring that allows you to investigate production issues and also provide a broad range of information about your service.

我们发现以下指南有助于度量指标的实施。这些指标应提供合理的监控，使您可以排查生产问题，并提供有关您服务更详细的信息。

### Intended Changes
### 预期的变化

When diagnosing an SLO-based alert, you need to be able to move from alerting metrics that notify you of user-impacting issues to metrics that tell you what is causing these issues. Recent intended changes to your service might be at fault. Add monitoring that informs you of any changes in production. To determine the trigger, we recommend the following:

在诊断基于SLO的警报时，您需要能够从通知您用户影响问题的警报指标转移到告知您导致这些问题的指标。 您服务的最新的更改可能导致新的错误。添加监控，通知您生产中的任何变化。要确定触发器，我们建议如下：

* Monitor the version of the binary.
* Monitor the command-line flags, especially when you use these flags to enable and disable features of the service.
* If configuration data is pushed to your service dynamically, monitor the version of this dynamic configuration.

* 监控二进制文件的版本。
* 监控命令行标志, 特别是当您使用这些标志启用和禁用服务功能时。
* 如果将配置数据动态推送到服务, 则监控此动态配置的版本。

If any of these pieces of the system aren’t versioned, you should be able to monitor the timestamp at which it was last built or packaged.

如果系统中的某个服务未进行版本控制, 那您应该监控上次生成或打包的时间戳。

When you’re trying to correlate an outage with a rollout, it’s much easier to look at a graph/dashboard linked from your alert than to trawl through your CI/CD (continuous integration/continuous delivery) system logs after the fact.

当您尝试将停机与部署相关联时, 查看与警报链接的图表/仪表板要比在事后通过CI / CD（持续集成/持续交付）系统日志更容易。

### Dependencies
### 依赖

Even if your service didn’t change, any of its dependencies might change or have problems, so you should also monitor responses coming from direct dependencies.

即使您的服务没有更改，其任何依赖项都可能变化或出现问题，因此您还应该监控来自直接依赖项的响应。

It’s reasonable to export the request and response size in bytes, latency, and response codes for each dependency. When choosing the metrics to graph, keep the four golden signals in mind. You can use additional labels on the metrics to break them down by response code, RPC (remote procedure call) method name, and peer job name.

根据每个依赖项的字节，延迟和响应代码导出请求和响应大小是合理的。在选择要绘制的指标时，请牢记四个黄金指标。您可以在度量标准上使用其他标签，以通过响应代码，RPC（远程过程调用）方法名称和对应业务名称将其分解。

Ideally, you can instrument the lower-level RPC client library to export these metrics once, instead of asking each RPC client library to export them. Instrumenting the client library provides more consistency and allows you to monitor new dependencies for free.

理想情况下，您可以制作一次底层RPC客户端库来导出他们的度量指标到仪表盘，而不是要求每个RPC客户端库导出它们。 规范客户端库提供了更高的一致性，并允许您自动监控新的依赖项。

You will sometimes come across dependencies that offer a very narrow API, where all functionality is available via a single RPC called Get, Query, or something equally unhelpful, and the actual command is specified as arguments to this RPC. A single instrumentation point in the client library falls short with this type of dependency: you will observe a high variance in latency and some percentage of errors that may or may not indicate that some part of this opaque API is failing entirely. If this dependency is critical, you have a couple of options to monitor it well:

您有时会遇到提供非常有限的API的依赖项，其中所有功能都可通过名为Get，Query或某些等于无效的单个RPC获得，并且实际命令被指定为此RPC的参数。客户端库中的单个检测点与此类依赖关系不符：您将观察到延迟的高低变化和一些百分比的错误，这些错误可能可以也可能不可以表明此API的某些部分完全失败。 如果这种依赖关系很重要，那么您有几个选择可以很好地监控它：

* Export separate metrics to tailor for the dependency, so that the metrics can unpack requests they receive to get at the actual signal.
* Ask the dependency owners to perform a rewrite to export a broader API that supports separate functionality split across separate RPC services and methods.

* 导出单独的度量值为依赖项量身定做, 以便这些度量值可以解释收到的请求以获取实际信号。
* 请依赖项所有者重构以支持导出单独的 RPC 服务和方法功能分离的更通用的 API。

### Saturation
### 饱和度

Aim to monitor and track the usage of every resource the service relies upon. Some resources have hard limits you cannot exceed, like RAM, disk, or CPU quota allocated to your application. Other resources—like open file descriptors, active threads in any thread pools, waiting times in queues, or the volume of written logs—may not have a clear hard limit but still require management.

监控和跟踪服务所依赖的每种资源的使用情况是最终目标。某些资源具有您不能超过的硬性限制，例如分配给您的应用程序的RAM，磁盘或CPU配额。 其他资源（如打开文件描述符，任何线程池中的活动线程，队列等待时间或写入的日志量）可能没有明确的硬限制，但仍需要管理。

Depending on the programming language in use, you should monitor additional resources:

根据使用的编程语言，您应该监控额外的资源：

* In Java: The heap and metaspace size, and more specific metrics depending on what type of garbage collection you’re using
* In Go: The number of goroutines

* 在 Java 中: 堆和 metaspace 大小, 以及更具体的度量指标, 具体取决于您使用的垃圾回收机制
* 在 Go 中: goroutines 的数量

The languages themselves provide varying support to track these resources.

这些语言本身为跟踪这些资源提供了不同的支持。

In addition to alerting on significant events as described in Chapter 5 you might also need to set up alerting that fires when you approach exhaustion for specific resources, such as:

除了在5章中所述的重大事件上发出报警外, 您还可能需要为特定资源设置报警, 例如:

* When the resource has a hard limit
* When crossing a usage threshold causes performance degradation

* 当资源有硬件限制时
* 当超过使用阈值时会导致性能下降

You should have monitoring metrics to track all resources—even resources that the service manages well. These metrics are vital in capacity and resource planning.

您应该拥有监控指标来跟踪所有资源 - 甚至在资源处于服务管理良好状态下。 这些指标在容量和资源规划中至关重要。

### Status of Served Traffic
### 服务通信状态

It’s a good idea to add metrics or metric labels that allow the dashboards to break down served traffic by status code (unless the metrics your service uses for SLI purposes already include this information). Here are some recommendations:

最好添加度量值或度量标签, 以允许仪表板按状态码区分服务的请求量 (除非服务用于 SLI 目的的度量值已经包括此信息)。以下是一些建议:

* For HTTP traffic, monitor all response codes, even if they don’t provide enough signal for alerting, because some can be triggered by incorrect client behavior.
* If you apply rate limits or quota limits to your users, monitor aggregates of how many requests were denied due to lack of quota.

* 对于 HTTP 请求, 监控所有响应状态码, 即使它们没有提供足够的警报信息, 因为有些可能是由错误的客户端行为触发的。
* 如果您对用户设置速率限制或配额限制, 则监控由于缺少配额而拒绝多少请求的统计。

Graphs of this data can help you identify when the volume of errors changes noticeably during a production change.

此数据的图表可以帮助您识别在生产环境更新迭代过程中出现异常错误的时间。

### Implementing Purposeful Metrics
目标度量指标实现

Each exposed metric should serve a purpose. Resist the temptation of exporting a handful of metrics just because they are easy to generate. Instead, think about how these metrics will be used. Metric design, or lack thereof, has implications.

每个导出的指标都应该有作用。不要因为生成简单而只导出少量的指标。相反，请考虑如何使用这些指标。 度量指标的设计或缺乏设计将会影响您的监控系统。

Ideally, metric values used for alerting change dramatically only when the system enters a problem state, and don’t change when a system is operating normally. On the other hand, metrics for debugging don’t have these requirements—they’re intended to provide insight about what is happening when alerts are triggered. Good debugging metrics will point at some aspect of the system that’s potentially causing the issues. When you write a postmortem, think about which additional metrics would have allowed you to diagnose the issue faster.

理想情况下，用于报警的度量指标仅在系统出现问题状态时发生显着变化，并且在系统正常运行时不会发生变化。另一方面，调试指标没有这些要求 - 它们旨在提供有关触发警报时发生的情况的原因。良好的调试指标将指向系统可能导致问题的某些方面。编写事后调查时，请考虑哪些其他指标可以让您更快地诊断问题。


